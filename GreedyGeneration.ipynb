{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47214232",
   "metadata": {},
   "source": [
    "## Greedy generation with predicted vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14212288",
   "metadata": {},
   "source": [
    "### Load in the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f541349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "models_path = \"models/\"\n",
    "model_name_1 = \"opus-mt-NORTH_EU-NORTH_EU\"\n",
    "model_name_2 = \"opus-mt-SCANDINAVIA-SCANDINAVIA\"\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "tokenizer_1 = MarianTokenizer.from_pretrained(models_path + model_name_1)\n",
    "model_1 = MarianMTModel.from_pretrained(models_path + model_name_1, output_hidden_states=True)\n",
    "\n",
    "tokenizer_2 = MarianTokenizer.from_pretrained(models_path + model_name_2)\n",
    "model_2 = MarianMTModel.from_pretrained(models_path + model_name_2, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8776896e",
   "metadata": {},
   "source": [
    "### Use pretrained MLP regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30186691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import jdata as jd\n",
    "import joblib\n",
    "\n",
    "# Load the MLP conf from the json file\n",
    "mlp_conf = jd.load(\"models/MLP_regressor_8192_sv_500k_opus-mt-NORTH_EU-NORTH_EU_to_opus-mt-SCANDINAVIA-SCANDINAVIA.json\")\n",
    "\n",
    "# Initialize MLP regressor\n",
    "mlp_regressor = MLPRegressor(random_state=1, hidden_layer_sizes=(8192))\n",
    "\n",
    "# Load in the regressor parameters and conf\n",
    "mlp_regressor.intercepts_ = mlp_conf[\"intercepts_\"]\n",
    "mlp_regressor.coefs_ = mlp_conf[\"coefs_\"]\n",
    "mlp_regressor.n_layers_ = mlp_conf[\"n_layers_\"]\n",
    "mlp_regressor.out_activation_ = mlp_conf[\"out_activation_\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de163c8",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa23e18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoded_str(tokenizer, model, input_str):\n",
    "    # create ids of encoded input vectors\n",
    "    input_ids = tokenizer(input_str, return_tensors=\"pt\").input_ids\n",
    "    # create BOS token\n",
    "    bos_id = tokenizer(\"<pad>\", add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "    assert bos_id[0, 0].item() == model.config.decoder_start_token_id, \"`decoder_input_ids` should correspond to `model.config.decoder_start_token_id`\"\n",
    "\n",
    "    # Get encoded sequence\n",
    "    outputs = model(input_ids, decoder_input_ids=bos_id, return_dict=True)\n",
    "    encoded_sequence = outputs.encoder_last_hidden_state\n",
    "    \n",
    "    return encoded_sequence\n",
    "\n",
    "def greedy_generate(model, encoded_sequence, tokenizer):\n",
    "    # Assign the BOS token as the first generated token for the decoder\n",
    "    decoder_input_ids = tokenizer(\"<pad>\", add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # Generate next tokens in loop, pick next token by greedy search\n",
    "    for _ in range(model_1.config.max_length):\n",
    "        lm_logits = model(None, encoder_outputs=(encoded_sequence,), decoder_input_ids=decoder_input_ids, return_dict=True).logits\n",
    "        next_decoder_input_ids = torch.argmax(lm_logits[:, -1:], axis=-1)\n",
    "        decoder_input_ids = torch.cat([decoder_input_ids, next_decoder_input_ids], axis=-1)\n",
    "        \n",
    "        if decoder_input_ids[0][-1] == 0:\n",
    "            return decoder_input_ids[0]\n",
    "        \n",
    "    #raise RuntimeError(f\"Generation did not finish after max iter or {model_1.config.max_length}\")\n",
    "    print(\"Generation did not finish\")\n",
    "    return decoder_input_ids[0]\n",
    "    \n",
    "def generate_with_different_encoding(\n",
    "    model_1, tokenizer_1, \n",
    "    model_2, tokenizer_2,\n",
    "    predictor,\n",
    "    input_str):\n",
    "    \"\"\"\n",
    "    Uses the model 1 encoder to encode the string, then using the predictor, \n",
    "    converts it to suitable embeding for model 2 and generates translated text.\n",
    "    \n",
    "    The target language token and ending token for model 2 are not predicted.\n",
    "    They are taken from model 2 encoding.\n",
    "    \"\"\"\n",
    "    # Encode the string with both models\n",
    "    encoded_sequence = get_encoded_str(tokenizer_1, model_1, input_str)\n",
    "    encoded_sequence_2 = get_encoded_str(tokenizer_2, model_2, input_str)\n",
    "\n",
    "    # Get the target language token encoded from model 2\n",
    "    target_lang_token_encoded = encoded_sequence_2[:1, :1, :]\n",
    "    # Get the ending token encoded from model 2\n",
    "    ending_token_encoded = encoded_sequence_2[:1, -1:, :]\n",
    "\n",
    "    # Predict the encoded sequence for model 2\n",
    "    predicted_embedding = predictor.predict(encoded_sequence.detach().numpy()[0][1:-1])\n",
    "    \n",
    "    # Reshape the embeding and convert it to tensor from np array\n",
    "    predicted_embedding = torch.Tensor(predicted_embedding.reshape(1, -1, encoded_sequence_2.shape[-1]))\n",
    "\n",
    "    # Add encoded target language token\n",
    "    predicted_embedding = torch.cat([target_lang_token_encoded, predicted_embedding], axis=1)\n",
    "    # Add encoded ending token\n",
    "    predicted_embedding = torch.cat([predicted_embedding, ending_token_encoded], axis=1)\n",
    "    # Generate the translated string\n",
    "    return greedy_generate(model_2, predicted_embedding, tokenizer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c6826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \">>da<< Vad gÃ¶r du?\"\n",
    "\n",
    "translated_model_1 = model_1.generate(**tokenizer_1(input_str, return_tensors=\"pt\", padding=True), max_new_tokens=512)[0]\n",
    "translated_model_2 = model_2.generate(**tokenizer_2(input_str, return_tensors=\"pt\", padding=True), max_new_tokens=512)[0]\n",
    "\n",
    "generated = generate_with_different_encoding(model_1, tokenizer_1, model_2, tokenizer_2, mlp_regressor, input_str)\n",
    "\n",
    "print(f\"Translated with model 1:           {tokenizer_1.decode(translated_model_1, skip_special_tokens=True)}\")\n",
    "print(f\"Translated with model 2:           {tokenizer_2.decode(translated_model_2, skip_special_tokens=True)}\")\n",
    "print(f\"Generated with predicted embeding: {tokenizer_2.decode(generated, skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9f0d26",
   "metadata": {},
   "source": [
    "### Mesure the performance of generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1178858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "from src import DataLoader\n",
    "\n",
    "def measure_translation_quality(lang_token, lang_1_path, lang_2_path, n_examples):\n",
    "    lang_1, lang_2 = DataLoader.read_parallel_corpus(\n",
    "        lang_1_path,\n",
    "        lang_2_path,\n",
    "        rows=1012)\n",
    "\n",
    "    # Add target language token to input string\n",
    "    lang_1 = [lang_token + i for i in lang_1]\n",
    "\n",
    "    translated_model_1 = [\n",
    "        tokenizer_1.decode(\n",
    "            model_1.generate(**tokenizer_1(input_str, return_tensors=\"pt\", padding=True), max_new_tokens=512)[0], \n",
    "            skip_special_tokens=True) \n",
    "        for input_str in lang_1]\n",
    "\n",
    "    translated_model_2 = [\n",
    "        tokenizer_2.decode(\n",
    "            model_2.generate(**tokenizer_2(input_str, return_tensors=\"pt\", padding=True), max_new_tokens=512)[0], \n",
    "            skip_special_tokens=True) \n",
    "        for input_str in lang_1]\n",
    "\n",
    "    translated_predicted_embeddings = [\n",
    "        tokenizer_2.decode(\n",
    "           generate_with_different_encoding(model_1, tokenizer_1, model_2, tokenizer_2, mlp_regressor, input_str), \n",
    "            skip_special_tokens=True) \n",
    "        for input_str in lang_1]\n",
    "\n",
    "    bleu = BLEU()\n",
    "\n",
    "    print(f\"Model 1 {model_name_1}: {bleu.corpus_score(translated_model_1, [lang_2])}\")\n",
    "    print(f\"Model 2 {model_name_2}: {bleu.corpus_score(translated_model_2, [lang_2])}\")\n",
    "    print(f\"Model 1 embedings transformed to model 2: {bleu.corpus_score(translated_predicted_embeddings, [lang_2])}\")\n",
    "    \n",
    "    for i in range(n_examples):\n",
    "        print()\n",
    "        print(f\"\\t Input:     {lang_1[i]}\")\n",
    "        print(f\"\\t Real:      {lang_2[i]}\")\n",
    "        print(f\"\\t Model 1:   {translated_model_1[i]}\")\n",
    "        print(f\"\\t Model 2:   {translated_model_2[i]}\")\n",
    "        print(f\"\\t Predicted: {translated_predicted_embeddings[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a58b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Translating from Swedish to Danish on the OpenSubtitles dataset.\")\n",
    "\n",
    "measure_translation_quality(\n",
    "    \">>da<< \",\n",
    "    \"data/da-sv.txt/OpenSubtitles.da-sv.sv\", \n",
    "    \"data/da-sv.txt/OpenSubtitles.da-sv.da\",\n",
    "    0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1498673",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Translating from Swedish to Danish on the FLORES-200 dataset.\")\n",
    "\n",
    "measure_translation_quality(\n",
    "    \">>da<<\",\n",
    "    \"data/flores200_dataset/devtest/swe_Latn.devtest\", \n",
    "    \"data/flores200_dataset/devtest/dan_Latn.devtest\",\n",
    "    0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a629002",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Translating from Danish to Swedish on the OpenSubtitles dataset.\")\n",
    "\n",
    "measure_translation_quality(\n",
    "    \">>sv<< \",\n",
    "    \"data/da-sv.txt/OpenSubtitles.da-sv.da\",\n",
    "    \"data/da-sv.txt/OpenSubtitles.da-sv.sv\", \n",
    "    0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e495d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Translating from Danish to Swedish on the FLORES-200 dataset.\")\n",
    "\n",
    "measure_translation_quality(\n",
    "    \">>sv<<\",\n",
    "    \"data/flores200_dataset/devtest/dan_Latn.devtest\",\n",
    "    \"data/flores200_dataset/devtest/swe_Latn.devtest\",\n",
    "    0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
